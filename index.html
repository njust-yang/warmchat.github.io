<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AvatarControl: Consistent Text-Driven Talking Head Avatar Editing with Optical Flow Guidance</title>
  <link rel="icon" type="image/x-icon" href="static/images/WIS.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: rgb(255, 105, 180);">A</span><span style="color: rgb(255, 223, 0);">v</span><span style="color: rgb(255, 182, 193);">a</span><span style="color: rgb(183, 110, 121);">t</span><span style="color: rgb(255, 127, 80);">a</span><span style="color: rgb(250,119,182);">r</span><span style="color: rgb(75, 54, 232);">C</span><span style="color: rgb(255, 127, 80);">o</span><span style="color: rgb(183, 110, 121);">n</span><span style="color: rgb(34, 110, 121);">t</span><span style="color: rgb(183, 34, 121);">r</span><span style="color: rgb(183, 110, 100);">o</span><span style="color: rgb(183, 110, 121);">l</span>: Consistent Text-Driven Talking Head Avatar Editing with Optical Flow Guidance</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank">author1</a>,
                <span class="author-block">
                  <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank">author2</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank">author3</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank">author4</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank">author5</a><sup>*</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-6 publication-authors">
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>

                  <div class="is-size-3 publication-authors">
                    <span class="author-block">Under Submission</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href='https://njust-yang.github.io/avatarcontrol.github.io/' target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper(Coming soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary(Coming soon)</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(Coming soon)</span>
                  </a>
                </span>
              
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://njust-yang.github.io/avatarcontrol.github.io/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(Coming soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video preload="auto"poster="" id="tree" autoplay controls muted loop width="600px" outline="0px"> 
        <!-- Your video -->
<!--         <source src="pics/teaser.mp4" -->
        <source src="pics/speaking.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered"> 
      </h2> -->
    </div>
  </div> 
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pre-trained conditional diffusion models have demonstrated
impressive potential in the editing of real-world images. Although these methods achieve reasonable editing effects, they
still suffer from inconsistencies for facial images in aspects
of appearance, expression, and temporal coherence. These issues arise from the independent editing of single images and
the inherent loss of temporal patterns of the editing process.
In this paper, we introduce AvatarControl, a novel framework
for editing and generating high-fidelity talking head avatars.
Instead of performing independent image editing, AvatarControl leverages the correspondence represented by optical flow
across views and frames to boost the editing consistency.
Specifically, given a set of talking avatar images rendered by a
pretrained 3D Gaussian Splatting model, we propose a multi-view consistency module to select reference views based on
optical flow. These selected frames are then utilized to provide complementary view-consistency for the target image,
unifying the edited appearance. Further, we propose a temporal consistency module, which applies the optical flow and
depth map changes of source images to the diffusion process,
fine-tuning the edited talking head images for better temporal
stability. Extensive experiments demonstrate that AvatarControl outperforms state-of-the-art methods in terms of appearance, expression, and temporal consistency in edited avatars.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper poster -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Methodology</h2>
        <div class="has-text-centered">
          <img src="./pics/pipeline.png" alt="pipeline" style="max-width: 100%; height: auto;">
        </div>
        <h2 class="subtitle has-text-justified">
          AvatarControl begins with the highly effective 3DGS model, which provides source render images and
depths (stage 1). To ensure multi-view consistency, we integrate the depths and landmarks extracted using the pre-trained
EMOCA with ControlNet, thereby maintaining both geometric and expression consistency. We then utilize optical flow changes
from the source render images to identify valuable reference views for the current editing frame, applying an attention alignment
strategy to further improve multi-view consistency (stage 2). Finally, we use the optical flow changes and depth variations from
the source render images to develop a temporally consistent diffusion model, which generates the final edited results (stage 3).
        </h2>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="comparison">
  <div class="container is-max-desktop" style="text-align: center;">
    <div class="hero-body">
     <h2 class="title is-3">Comparisons with Video Editing Methods</h2>
      <video preload="auto"poster="" id="tree" autoplay controls muted loop width="600px" outline="0px"> 
        <!-- Your video -->
<!--         <source src="pics/comparison.mp4" -->
        <source src="pics/comparison.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered"> 
      </h2> -->
    </div>
  </div> 
</section>

  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">The Editing Results of Our Method</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-comp_avatar_1">
            <video poster="" id="comp_avatar_1" autoplay controls muted loop playsinline height="100%">
              <source src="./pics/demo1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-comp_avatar_2">
            <video poster="" id="comp_avatar_2" autoplay controls muted loop playsinline height="100%">
              <source src="./pics/demo2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-comp_avatar_3">
            <video poster="" id="comp_avatar_3" autoplay controls muted loop playsinline height="100%">
              <source src="./pics/demo3.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script>
      window.addEventListener('DOMContentLoaded', (event) => {
        const videoWrappers = document.querySelectorAll('.video-wrapper');
      
        videoWrappers.forEach(wrapper => {
          const defaultVideo = wrapper.querySelector('.default-video');
          const aspectRatio = defaultVideo.videoWidth / defaultVideo.videoHeight;
          const height = wrapper.offsetWidth / aspectRatio;
      
          wrapper.style.height = `${height}px`;
      
          wrapper.addEventListener('mouseenter', () => {
            defaultVideo.pause();
            hoverVideo.play();
          });
      
          wrapper.addEventListener('mouseleave', () => {
            defaultVideo.play();
            hoverVideo.pause();
          });
        });
      }); 
      $(document).ready(function() {
        var carouselItems = $('.carousel .item');
        var numItems = carouselItems.length;
        var numVideos = 5;
        var currentIndex = 0;
    
        $('.carousel').on('click', function() {
          currentIndex++;
          if (currentIndex + numVideos <= numItems) {
            carouselItems.removeClass('active');
            carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
          } else {
            currentIndex = 0;
            carouselItems.removeClass('active');
            carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
          }
        });
    
        carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
      });
    </script>
  </body>
  </html>
